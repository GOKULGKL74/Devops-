pod -app -starting point
node -master node,slave node 
cluster -n of node

i.k3Drawbacks of dockers:
  -single host 
  -no auto healing - devoper keep track whch container s down 
  -no auto scale 
  -simple platform -no* enterprise level support  eg:no loadbalancer,no firewal ,no auto scale,no api gateway.....
docker cant be a prod bz no enterprise support - may be docker swan useful 

  solution:-k8s
K8S(google) -pods 
  -container orchestration platform
  -its a cluster (group of nodes)master-slave 
  -replica sets to solve auto scale
  -fix/control the damage -if any conatainer down by automatic creates new container    -for auto heal
  -hav multiple enterprise platform support called borg* BY GOOGLE -so its preferable
....
cluster
  -grp of pods
    pod-grp of container
.....
pods: starts here 
  1.define hw to run
  2.pod(one r more containers(one fr app /one fr yaml to execute))
        easy to talk via localhost,share local storage,share netwrk,volume
  y yaml? bz it was standard for enterprise
Terms:
-k8s starts with pod.yaml   
-kubectl -command line for k8s

drwbacks:still no advanced loadbalancer ,Still people prefer virtual machine 

Architecture
control plane  / data plane
   => Master (control plane)
   =>slave/worker (data plane)
slave(3):
--->pod - smallest deployable unit -container run time/wrapper 
--->hav container run time-container D,crio,docker swim  //to run app
--->*kube-let-responsible for run app if issue it will inform kubernetes/control plane nd take necessary action 
--->kube proxy -netwrking, ip ,load balncer -mandatory -every pod has ip -can talk another node

master (control)
--->Api server(heart) -core component -validate the request nd routhrm them -take instructions from external world 
--->etcd -key value pair -cluster related info(backup)
--->control manager -contollers -used fr created update del
--->scheduler-schedule the resource Like go to node1 /2(isntruction gvn by api server)

--->C-C-M (CLOUD CONTROL MANAGER) -

K8s Distribution: have support (better user expr)-some popular 
  k8s-if time is not dependent 
  OPENSHIFT-redhat
  RANCHER -rancher labs
  Tanzo -vmware
  EKS -amazon
  AKS

demo-manage kops
1.create EC2
2.Install dependies like kubectl,python nd kubectl
3give permision if admin fine else fice IAM user permission(aws configure)
4.instal s3 -for storing kops objects (back up)
5.create cluster



minikube
-->install kubectl //cmdline
-->install minikube   //used to create k8s cluster
--->minikube start
      -vm created
      -above vm single node k8s cluster
  ->create yaml file for config
-->kubectl create -f pod.yaml
-->minukube ssh 
-->curl ip
.................................deployment.............................
deployemnt -hav auto scale ,auto healing
  -->pod -- deployment(.yml)gv count for replica  --  Replica set(controller)  -->pods
    controller -maintain desired state=actual state  count of replica 
    deployemnt- wrapper

controller -default controller,RgoCD

DEMO:   deploy-replicas-pod  :auto healing
  -deployment.yml   //add replicas as 1
  -kubectl apply -f pod.yml     //created(3)
        deploy also created pods also created rs also created 
  -kubectl get pods    kubectl get deploy   kubectl get rs
  //kubectl get pods -w   //w-live action
if u delete 1 same time another pod wil create
  -minikube ssh   | ssh -i ip (remote)
.....................
SERVICE:
-load balancing
-lables nd selectors  //not depend on ip only lables thats mentioned in yaml for that nods
-expose to external wrld via load balancer 

Service types:
  node port  -only access inside org vpc -hav wrker node ip address
  load balancer-generqte elastic public ip -acess by everyone(outside wrld)-public 
              -ccm will generate public ip with help of cloud provider   ccm-cloud control manager
  cluster ip -only who hav access to wrker node /k8s cluster netwrk (inisde k8s)
             -has load balancing ,discovery

INNGRESS:
Y ingress?
  enterprise TLS load balancing -
          ratio based,black listing ,white listing,domain based....
  cloud provide chrge for ip bz of static ip

  ingress resourse ( load balancer cmpy creates ingress controller nd posted in github)
              load balancers:nginx,f5,ambasidor,HA proxy...
  user deploy ingress controller(ngnix) in k8s cluste nd depend on req define  ingress  yaml file define (white listing...) 
  ingress contoller watch for ingress resourse  nd provide the feature

eg: devlop an app http://myapp:8080     -pod
    http://192...:8080        -service -test case uselful in ip
    https:myapp                -ingress   -no ip no port
[browser to proxy -proxy to ingress -ingress to servce -servcw to pod]  //bare model
IMPLEMENTATON:need ingress controller -evaluation nd progress like redirections   
  
.........

RBAC:role based access control
-security
-define access 
-2 parts
    user      nd   service accounts
manage RBAC
  service acconts/users
  k8s roles/cluster roles
  role binding/cluster role binding
user:
-->k8s wont manage user managemnt
-->k8s offload the user managemnt to identity providers
   Api server works like OATH server
    eg:aws-if using EKS we have IAM users with IAM access provider-can login 
    keyClock is a popular identity broker with LDAP,SSO,OKTA
service:
-->by yaml file
by default servce r k8s attached to pod else we can user defined
how to manage?
 role  -gv role to the user like create,read
 role binding  -attach the role to user

role/cluster binding
 attacking the role to users 

use redhat for rbac
.....

CR : Custom Resource D
Fyi : deployment.yml file creates resources(version,netrwrking,strge)- resource definition (api server) validates the resources -deployment controller starts exec
    Virtual service.yml file creates custom resource -custom resource definition CRD validates the custom resource -NOW K8S FUNCTIONALITY EXTENDED-custom k8s contoller satrts exec
  extend feature(api) using
          CRD -custom resourc definition
              defining new type api and submit to k8s -in yaml file define what user can create
              if new type added directly in dep.yml show err does nt known  
              so custom resource submit to CRD (all possible senario)via yaml file nd k8s will approve to use 
----CR -resouce defintion validate the resourse which is accpeted by k8s
----CRD validate the the custom resource (CR)
----deployemnt controller creates pods,replica set so on..
like custom controller creates custom resourse
          CR
          Custom contoller  -wrte by golang

configmaps nd secrets
1.config-non sensitive like port
2.secrets-sensitive info like passwrd
          Encrypt
          giv strong RBAC rule-least privelage //only limited people can access

k8s Monitoring :Prometheus
k8s have API server ,it exposes lot of metrics about cluster
apiserver/metrices -to get status of cluster 

after installed prometheus 
-it fetch that metrices and stored in TSDB (Time series DB) nd stored in HDD/SDD Node
-with help of alertmanager sent notification
-Run PromQL query in promethous to get detailed info

GUI-Grafana  -data visulasation
y? bz it have json data difficult to check logs so Grafana

Helm/operators -package maanger fr k8s
-





